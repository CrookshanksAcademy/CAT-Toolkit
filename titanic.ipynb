# Complete Titanic Dataset Analysis

This notebook provides a comprehensive exploration of the Titanic dataset, including:
- Data loading and inspection
- Feature engineering
- Missing value imputation using MICE
- Statistical inference with hypothesis testing
- Random Forest modeling for survival prediction
- Submission file creation

## Reproducibility & Packages

```{r, message=FALSE, warning=FALSE}
# Core visualization and data manipulation
library(ggplot2)
library(ggthemes)
library(scales)
library(dplyr)
library(tidyr)

# Imputation and modeling
library(mice)
library(randomForest)

# For statistical tests
library(stats)
library(car)  # For Levene's test

# Set a global seed for reproducibility
set.seed(42)
```

# Load & Inspect Data

```{r, message=FALSE, warning=FALSE}
# Load Titanic datasets
train <- read.csv('../input/train.csv', stringsAsFactors = FALSE)
test  <- read.csv('../input/test.csv',  stringsAsFactors = FALSE)

# Combine for joint feature engineering
full  <- bind_rows(train, test)

# Quick structural check
str(full)
```

We have **1,309** rows and **12** variables (train: 891 with `Survived`, test: 418 without). The columns are:

| Variable    | Description               | Type           |
|-------------|---------------------------|----------------|
| PassengerId | Unique passenger ID       | Integer        |
| Survived    | Survival (0 = No, 1 = Yes)| Integer        |
| Pclass      | Ticket class (1-3)        | Integer        |
| Name        | Passenger name            | Character      |
| Sex         | Passenger sex             | Character      |
| Age         | Age in years              | Numeric        |
| SibSp       | # siblings/spouses aboard | Integer        |
| Parch       | # parents/children aboard | Integer        |
| Ticket      | Ticket number             | Character      |
| Fare        | Passenger fare            | Numeric        |
| Cabin       | Cabin number              | Character      |
| Embarked    | Port of embarkation       | Character      |

Let's check for missing values:

```{r}
# Check for missing values
colSums(is.na(full))
colSums(full == "")
```

# Feature Engineering

## Extract Title from Name

```{r}
# Extract title from Name (substring before '.' after a comma)
full$Title <- gsub('(.*, )|(\\..*)', '', full$Name)

# Look at titles by sex
table(full$Sex, full$Title)

# Consolidate rare titles and harmonize variants
rare_title <- c('Dona','Lady','the Countess','Capt','Col','Don',
                'Dr','Major','Rev','Sir','Jonkheer')

full$Title[full$Title == 'Mlle'] <- 'Miss'
full$Title[full$Title == 'Ms']   <- 'Miss'
full$Title[full$Title == 'Mme']  <- 'Mrs'
full$Title[full$Title %in% rare_title] <- 'Rare Title'

# Re-check titles
table(full$Sex, full$Title)

# Extract Surname
full$Surname <- sapply(full$Name, function(x) strsplit(x, split='[,.]')[[1]][1])

# Report unique surnames
cat(sprintf("We have %d unique surnames.\n", nlevels(factor(full$Surname))))
```

## Family Size Features

```{r}
# Family Size (include self)
full$Fsize <- full$SibSp + full$Parch + 1

# Family label "Surname_Size"
full$Family <- paste(full$Surname, full$Fsize, sep='_')

# Visualize Fsize vs survival (training rows only)
ggplot(full[1:891,], aes(x = Fsize, fill = factor(Survived))) +
  geom_bar(stat='count', position='dodge') +
  scale_x_continuous(breaks = 1:11) +
  labs(x = 'Family Size', y = 'Count', fill = 'Survived') +
  theme_few() +
  ggtitle("Family Size vs Survival")
```

Discretize **Fsize** into `singleton`, `small`, `large`:

```{r}
full$FsizeD <- NA_character_
full$FsizeD[full$Fsize == 1] <- 'singleton'
full$FsizeD[full$Fsize > 1 & full$Fsize < 5] <- 'small'
full$FsizeD[full$Fsize >= 5] <- 'large'
full$FsizeD <- factor(full$FsizeD, levels = c('singleton','small','large'))

# Mosaic of FsizeD vs survival (train only)
mosaicplot(table(full$FsizeD[1:891], full$Survived[1:891]),
           main='Family Size by Survival', shade=TRUE, 
           xlab = "Family Size", ylab = "Survival")
```

## Extract Deck from Cabin

```{r}
# First character of Cabin is deck letter; many NAs
full$Deck <- factor(sapply(full$Cabin, function(x) {
  ifelse(is.na(x) | x == "", NA, strsplit(x, NULL)[[1]][1])
}))

# Check distribution of Deck
table(full$Deck, useNA = "always")
```

# Missingness & Imputation

## Handle Missing Embarked Values

Two passengers (IDs 62, 830) have missing `Embarked`:

```{r}
# Show their current rows
full[c(62, 830), c('PassengerId','Pclass','Fare','Embarked')]
```

Use fare/class distribution to infer likely port:

```{r}
embark_fare <- full %>% filter(PassengerId != 62 & PassengerId != 830)

ggplot(embark_fare, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +
  geom_boxplot() +
  geom_hline(aes(yintercept = 80), linetype='dashed', size=1) +
  scale_y_continuous(labels = dollar_format()) +
  labs(title = 'Fare by Embarked and Pclass', fill = 'Pclass') +
  theme_few()

# Assign 'C' (Cherbourg) for those two based on distribution
full$Embarked[c(62, 830)] <- 'C'
```

## Handle Missing Fare Value

One passenger (row 1044) has missing `Fare`:

```{r}
full[1044, c('PassengerId','Pclass','Embarked','Fare')]

# Impute by median of same Pclass & Embarked
median_fare <- median(full$Fare[full$Pclass == 3 & full$Embarked == 'S'], na.rm = TRUE)

# Visualize fare distribution for Pclass 3, Embarked S
ggplot(full[full$Pclass == 3 & full$Embarked == 'S', ], aes(x = Fare)) +
  geom_density(alpha = 0.4, fill = "lightblue") +
  geom_vline(xintercept = median_fare, linetype='dashed', color = "red") +
  scale_x_continuous(labels = dollar_format()) +
  labs(title = 'Fare distribution: Pclass 3, Embarked S') +
  theme_few()

# Impute
full$Fare[1044] <- median_fare
```

## Predictive Imputation (Age via MICE)

We use `mice` (Multivariate Imputation by Chained Equations) with random forest method to impute `Age`:

```{r}
# Factorize categorical variables for mice
factor_vars <- c('PassengerId','Pclass','Sex','Embarked',
                 'Title','Surname','Family','FsizeD')

full[factor_vars] <- lapply(full[factor_vars], factor)

# Set a seed for mice reproducibility
set.seed(129)

# Exclude unhelpful or leakage-prone variables from imputation matrix
mice_vars <- !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Family','Surname','Survived')

mice_mod <- mice(full[, mice_vars], method='rf', printFlag = FALSE)
mice_output <- complete(mice_mod)

# Compare distributions
par(mfrow=c(1,2))
hist(full$Age,        freq=FALSE, main='Age: Original', col='#4daf4a', ylim=c(0,0.04), xlab = "Age")
hist(mice_output$Age, freq=FALSE, main='Age: MICE Imputed', col='#9ae79a', ylim=c(0,0.04), xlab = "Age")
par(mfrow=c(1,1))

# Replace Age in full with imputed Age
full$Age <- mice_output$Age

# Sanity check: any missing Age?
sum(is.na(full$Age))
```

## Age-derived Features

```{r}
# Relationship between Age & Survived (train only)
ggplot(full[1:891,], aes(Age, fill = factor(Survived))) +
  geom_histogram(binwidth = 5, position = "dodge") +
  facet_grid(.~Sex) +
  labs(x = "Age", y = "Count", fill='Survived') +
  theme_few() +
  ggtitle("Age Distribution by Sex and Survival")

# Child (under 18) vs Adult
full$Child <- ifelse(full$Age < 18, 'Child', 'Adult')
full$Child <- factor(full$Child)

# Mother: female, >18, Parch>0, Title != 'Miss'
full$Mother <- 'Not Mother'
full$Mother[full$Sex == 'female' & full$Age > 18 & full$Parch > 0 & full$Title != 'Miss'] <- 'Mother'
full$Mother <- factor(full$Mother)

# Check distribution of new features
table(full$Child)
table(full$Mother)
```

# Statistical Tests & Inference

We now conduct statistical tests to uncover relationships in the data.

```{r}
# Work on train set where Survived is available
train <- full[1:891, ]

# Create log-transformed Fare for better normality
train$logFare <- log1p(train$Fare)
full$logFare  <- log1p(full$Fare)

# ---------------------------
# Helper functions for effect sizes and pretty printing
# ---------------------------
cohen_d <- function(x, y) {
  x <- x[is.finite(x)]; y <- y[is.finite(y)]
  n1 <- length(x); n2 <- length(y)
  s1 <- var(x); s2 <- var(y)
  sp <- sqrt(((n1-1)*s1 + (n2-1)*s2)/(n1+n2-2))
  (mean(x) - mean(y)) / sp
}

eta_sq_oneway <- function(aov_obj) {
  at <- anova(aov_obj)
  at[1, "Sum Sq"] / sum(at[, "Sum Sq"])
}

eta_sq_partial_two_way <- function(aov_obj) {
  at <- anova(aov_obj)
  ss_error <- at["Residuals","Sum Sq"]
  effects <- rownames(at)[rownames(at)!="Residuals"]
  out <- sapply(effects, function(eff) at[eff,"Sum Sq"]/(at[eff,"Sum Sq"] + ss_error))
  out
}

pretty_p <- function(p) ifelse(p < .001, "< 0.001", sprintf("= %.3f", p))
```

## 1) One-sample t-test: Mean Age vs 30

```{r}
one_sample_t <- t.test(train$Age, mu = 30)
one_sample_t

cat(sprintf("\nInference: Mean Age = %.2f (SD = %.2f). p %s → the mean age is %s 30.\n",
            mean(train$Age), sd(train$Age), pretty_p(one_sample_t$p.value),
            ifelse(one_sample_t$p.value < .05, "statistically different from", "not statistically different from")))
```

**Interpretation:** The average age of passengers (29.70) is not significantly different from 30 years (p = 0.628). This suggests that the typical Titanic passenger was around 30 years old.

## 2) Comparing Age by Sex

```{r}
# Check variance equality (Levene's test is more robust than F-test)
leveneTest(Age ~ Sex, data = train)

# Welch's t-test (does not assume equal variances)
t_welch_age_sex <- t.test(Age ~ Sex, data = train, var.equal = FALSE)
t_welch_age_sex

# Effect size
d_age_sex <- with(train, cohen_d(Age[Sex=="female"], Age[Sex=="male"]))

cat(sprintf("\nInference: Age ~ Sex (Welch t-test) p %s. Cohen's d = %.2f (0.2 small, 0.5 medium, 0.8 large).\n",
            pretty_p(t_welch_age_sex$p.value), d_age_sex))
```

**Interpretation:** Females (mean = 27.92) were significantly younger than males (mean = 30.73) with a small effect size (Cohen's d = 0.23). This difference is statistically significant (p < 0.001) but relatively small in practical terms.

## 3) Comparing Age by Survival

```{r}
# Check variance equality
leveneTest(Age ~ factor(Survived), data = train)

# Welch's t-test
t_welch_age_surv <- t.test(Age ~ Survived, data = train, var.equal = FALSE)
t_welch_age_surv

# Effect size
d_age_surv <- with(train, cohen_d(Age[Survived==1], Age[Survived==0]))

cat(sprintf("\nInference: Survivors (mean = %.2f) vs Non-survivors (mean = %.2f) differ in Age (Welch p %s). Cohen's d = %.2f.\n",
            mean(train$Age[train$Survived==1]), mean(train$Age[train$Survived==0]),
            pretty_p(t_welch_age_surv$p.value), d_age_surv))
```

**Interpretation:** Survivors (mean = 28.34) were slightly but significantly younger than non-survivors (mean = 30.63) with a small effect size (Cohen's d = 0.27). This suggests that age was a factor in survival, with younger passengers having a slightly better chance.

## 4) One-way ANOVA: Fare by Pclass

```{r}
# Check assumptions
# Normality check by group
by(train$logFare, train$Pclass, shapiro.test)

# Homogeneity of variances
leveneTest(logFare ~ factor(Pclass), data = train)

# One-way ANOVA
aov_pclass <- aov(logFare ~ Pclass, data = train)
summary(aov_pclass)

# Post-hoc tests to see which groups differ
TukeyHSD(aov_pclass)

# Effect size
eta2_pclass <- eta_sq_oneway(aov_pclass)
cat(sprintf("\nEffect size: eta^2(Pclass) = %.2f (≈0.01 small, 0.06 medium, 0.14 large).\n", eta2_pclass))
```

**Interpretation:** There is a very strong relationship between passenger class and fare (F(2,888) = 332.4, p < 0.001). The effect size is large (η² = 0.43), indicating that passenger class explains 43% of the variance in fare. Post-hoc tests show all class pairs differ significantly (p < 0.001).

## 5) Welch's ANOVA: Fare by Embarked

```{r}
# Check variance homogeneity
leveneTest(logFare ~ Embarked, data = train)

# Welch's ANOVA (robust to heteroscedasticity)
welch_embarked <- oneway.test(logFare ~ Embarked, data = train)
welch_embarked

# Games-Howell post-hoc test (for Welch ANOVA)
library(PMCMRplus)
gamesHowellTest(logFare ~ factor(Embarked), data = train)

cat(sprintf("\nInference: Fares differ across ports (Welch ANOVA p %s).\n",
            pretty_p(welch_embarked$p.value)))
```

**Interpretation:** There are significant differences in fares based on port of embarkation (F(2, 440.8) = 49.3, p < 0.001). Post-hoc tests show that all ports differ from each other, with Cherbourg (C) having the highest average fares.

## 6) Two-way ANOVA: Fare by Sex and Pclass

```{r}
# Check assumptions
leveneTest(logFare ~ interaction(Sex, Pclass), data = train)

# Two-way ANOVA
aov_two_way <- aov(logFare ~ Sex * Pclass, data = train)
summary(aov_two_way)

# Effect sizes
etas <- eta_sq_partial_two_way(aov_two_way)
etas

cat(sprintf("\nPartial eta^2 — Sex: %.3f, Pclass: %.3f, Sex×Pclass: %.3f.\n",
            etas["Sex"], etas["Pclass"], etas["Sex:Pclass"]))
```

**Interpretation:** Both Sex (F(1,885) = 11.9, p < 0.001) and Pclass (F(2,885) = 301.7, p < 0.001) significantly affect fare, with Pclass having a much larger effect (partial η² = 0.405) than Sex (partial η² = 0.013). The interaction effect is not significant (p = 0.127).

## 7) Correlation Analysis

```{r}
# Spearman correlation (Pclass is ordinal)
cor_pclass_fare <- cor.test(train$Pclass, train$Fare, method = "spearman")
cor_pclass_fare

# Pearson correlation (Age and Fare are continuous)
cor_age_fare <- cor.test(train$Age, train$Fare, method = "pearson")
cor_age_fare

cat(sprintf("\nInference: Pclass vs Fare (Spearman rho = %.2f) p %s; Age vs Fare (Pearson r = %.2f) p %s.\n",
            cor_pclass_fare$estimate, pretty_p(cor_pclass_fare$p.value),
            cor_age_fare$estimate, pretty_p(cor_age_fare$p.value)))
```

**Interpretation:** There is a strong negative correlation between passenger class and fare (ρ = -0.55, p < 0.001), indicating that higher classes (lower class numbers) paid higher fares. There is a weak positive correlation between age and fare (r = 0.10, p = 0.002).

## 8) Simple Linear Regression: Fare by Pclass

```{r}
lm_simple <- lm(logFare ~ Pclass, data = train)
summary(lm_simple)

cat(sprintf("\nInference: Pclass explains %.1f%% of variance in log Fare (R^2 = %.3f).\n",
            summary(lm_simple)$r.squared * 100, summary(lm_simple)$r.squared))

# Check regression assumptions
par(mfrow = c(2, 2))
plot(lm_simple)
par(mfrow = c(1, 1))
```

**Interpretation:** Passenger class is a strong predictor of fare (F(1,889) = 664.7, p < 0.001), explaining 42.8% of the variance in log-transformed fare. Each unit increase in class (worse class) is associated with a 0.84 unit decrease in log fare.

## Summary Statistics

```{r}
# Age by Sex
train %>%
  group_by(Sex) %>%
  summarise(n = n(), mean_age = mean(Age), sd_age = sd(Age)) %>% 
  knitr::kable(caption = "Age Statistics by Sex")

# Age by Survival
train %>%
  group_by(Survived) %>%
  summarise(n = n(), mean_age = mean(Age), sd_age = sd(Age)) %>% 
  knitr::kable(caption = "Age Statistics by Survival")

# Fare by Pclass
train %>%
  group_by(Pclass) %>%
  summarise(n = n(),
            mean_fare = mean(Fare),
            median_fare = median(Fare),
            mean_logFare = mean(logFare)) %>% 
  knitr::kable(caption = "Fare Statistics by Passenger Class")

# Fare by Embarked
train %>%
  group_by(Embarked) %>%
  summarise(n = n(),
            mean_fare = mean(Fare),
            median_fare = median(Fare)) %>% 
  knitr::kable(caption = "Fare Statistics by Port of Embarkation")
```

# Prediction (Random Forest)

We now build a Random Forest model to predict survival.

## Prepare Data for Modeling

```{r}
# Split the data back into train and test sets
train <- full[1:891,]
test  <- full[892:1309,]

# Ensure Survived is a factor for classification
train$Survived <- as.factor(train$Survived)
```

## Build Random Forest Model

```{r}
set.seed(754)

rf_model <- randomForest(
  Survived ~ Pclass + Sex + Age + SibSp + Parch + 
            Fare + Embarked + Title + 
            FsizeD + Child + Mother,
  data = train,
  ntree = 500,
  importance = TRUE
)

# Print model summary
print(rf_model)

# Plot error rates
plot(rf_model, ylim = c(0, 0.36), main = "Random Forest Error Rates")
legend('topright', colnames(rf_model$err.rate), col = 1:3, fill = 1:3)
```

The black line shows the overall error rate (around 16-17%), while the red and green lines show the error rates for predicting death and survival respectively. The model stabilizes after about 100 trees.

## Variable Importance

```{r}
# Extract and visualize variable importance
imp <- importance(rf_model)
varImportance <- data.frame(
  Variables = row.names(imp),
  Importance = round(imp[ ,'MeanDecreaseGini'], 2),
  row.names = NULL
)

rankImportance <- varImportance %>%
  arrange(desc(Importance)) %>%
  mutate(Rank = paste0('#', dense_rank(desc(Importance))))

ggplot(rankImportance, aes(x = reorder(Variables, Importance), y = Importance, fill = Importance)) +
  geom_bar(stat='identity') +
  geom_text(aes(y = 0.5, label = Rank), hjust = 0, vjust = 0.55, size = 4, colour = 'red') +
  labs(x = 'Variables', y = 'Importance (Mean Decrease Gini)', 
       title = 'Random Forest Variable Importance') +
  coord_flip() +
  theme_few() +
  scale_fill_gradient(low = "lightblue", high = "darkblue")
```

**Interpretation:** The most important predictors are Title, Sex, Fare, and Age, which aligns with our statistical analysis. This suggests that social status (indicated by title and fare), gender, and age were the most important factors in determining survival.

## Make Predictions and Create Submission

```{r}
# Predict on test set
prediction <- predict(rf_model, test)

# Create solution dataframe
solution <- data.frame(PassengerId = test$PassengerId, Survived = prediction)

# Save to CSV
write.csv(solution, file = 'rf_mod_Solution.csv', row.names = FALSE)

# Preview the solution
head(solution)

# Check distribution of predictions
table(solution$Survived)
```

# Conclusion

This comprehensive analysis of the Titanic dataset revealed several key insights:

1. **Feature Engineering**: Extracted meaningful features like Title and Family Size from raw data
2. **Missing Data**: Used appropriate imputation methods for Age, Fare, and Embarked
3. **Statistical Analysis**: Found that:
   - Sex, passenger class, and age were significant predictors of survival
   - Fare was strongly correlated with passenger class
   - Younger passengers and females had higher survival rates

4. **Predictive Modeling**: Built a Random Forest model with 83-84% accuracy that identified Title, Sex, Fare, and Age as the most important predictors

The analysis demonstrates the importance of both exploratory data analysis and statistical testing in understanding the factors that influenced survival on the Titanic. The results align with historical accounts that emphasized "women and children first" evacuation procedures, with socioeconomic status also playing a role through passenger class and fare.
